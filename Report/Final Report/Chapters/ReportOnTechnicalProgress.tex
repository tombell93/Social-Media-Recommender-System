\chapter{Report on Technical Progress}

This chapter highlights my progress so far, by explaining my theoretical findings and highlighting my implementation progress. 

\section{Research Progress}

A user's preferences will be modelled as a tuple $U$ of attributes $u_k$ which gives the users preference for each topic. This is the user profile.

In order to rank data according to its relevance, it must first be scored according to its relevance. Scoring uses a recommendation system in which an item of data is allocated a tuple of attribute scores. These are compared to the user's profile to give the item a score.

Using the unweighted scoring rule

\begin{equation}\label{AverageUnweightedRule}	
	f(D) = \frac{\sum_{i=1}^{i} d_i}{n}
\end{equation}

where $D$ is our item of data, and the elements $d_i$ are its topic-attributes, Fagin and Wimmers' \cite{FaginWimmers1} conversion is used for incorporating weights into a scoring rule, no matter the nature of the scoring rule. It takes a function $f$ which describes an unweighted rule for applying a score to a tuple of $n$ entries (attributes in our case) and gives a weighted rule based on $f$ which is compatible with any rule. 

They take a set $\Theta = (\theta_1,\theta_2,\dotsc,\theta_n)$ of weights which relate the effect of an entry $x$ on the score of the tuple $ X = (x_1, x_2, \dots, x_n)$. If $f(X)$ is the unweighted scoring rule, then for a tuple of $m$ entries

\begin{equation}\label{WeightedRule}	
	f_\Theta (X) = \left(\sum_{i=1}^{m-1} i(\theta_{\sigma{(i)}} - \theta_{\sigma{(i+1)}})\times f(X\upharpoonright\left\{\sigma(1),\dots,\sigma(i)\right\})\right) + m  \theta_\sigma{(m)}  f(X)
\end{equation}

Here $X\restriction\left\{\sigma(1),\dots,\sigma(i)\right\}$ is a restriction of $X$ to the domain of a bijection $\sigma$ which orders the weightings to match the order of entries in the tuple. In our case this bijection would be a mapping from each attribute of an item of data to the weighting associated with that attribute. The weighting $\Theta$ would be dependent upon a complementary tuple describing the user, i.e. their attribute preferences. The term $(\theta_{\sigma{(i)}} - \theta_{\sigma{(i+1)}})$ is the difference between the weightings of two consecutive entries. 
Using this conversion, the following can be derived

\begin{align}\label{OurWeightedRuleDerivation}
f_U (D) &= \left(\sum_{n=1}^{M-1} n (u_{\sigma{(n)}} - u_{\sigma{(n+1)}})\times \frac{\sum_{i=1}^{n} d_i}{n}\right) + M \times u_\sigma{(M)} \times \frac{\sum_{i=1}^{n} d_i}{n} 
\\ &= (u_1-u_2)d_1 + 2(u_2-u_3)\frac{d_1+d_2}{2} + 3u_3\frac{d_1+d_2+d_3}{3}
\\ &= u_1d_1 + u_2d_2 + \dots + u_Md_M
\end{align}

This led to the proof of an intuitive linear weighted scoring rule (Eqn. \ref{BasicScoringRule}).

\begin{equation}\label{BasicScoringRule}
f_U (D) = \sum_{k=1}^{M-1} u_kd_k
\end{equation}

As items are received, they are scored and ordered, however an item may be relevant in terms of topic, but not in terms of age since its creation. It may have been created a long time ago and thus needs removing to make way for new items. Here, an item-deletion term ${\mathrm{e}}^{-\alpha t(d)}$ is added, where $t(d)$ is the minutes lapsed since the receiving of item $d$ and $\alpha$ is a delay coefficient. As time increases, the item-removal term tends to zero. This gives us

\begin{equation}\label{BasicScoringRule2}
	f_U (D) = {\mathrm{e}}^{-\alpha t(d)} \times \sum_{k=1}^{m-1} u_kd_k
\end{equation}

This exponential deletion function is used as it keeps new items at the top for longer. A polynomial could have been used with similar effects, but a linear decay would have cause the item to fall down the ranked list too quickly.
Time-relevant data is scored differently to topic-relevant data. Time scoring may be done using a continuous increasing term whose value is related to the time before a deadline approaches. $t_{threshold}$ is the time at which time-related scoring begins, $t_{tillDue}(D)$ is the time until item D is due and $\beta$ is a normalisation parameter. It must be set such that at the point at which D becomes relevant in time, $f_{time}(D)$ is greater than the maximum non-time-related score. This gives us

\begin{equation}\label{BasicTimeScoringRule}
	f_{time} (D) = {\mathrm{e}}^{\beta (t_{threshold}-t_{tillDue}(D))}
\end{equation}

The exponential decay rate, similar to that of entropy in information theory is used to cause the rate at which score increases to increase as the deadline approaches. 
Here, a parameter $\gamma$ is introduced to control the relative weight between the topic-scoring term and the time-based scoring term. A static look-up matrix may be used to find the value of $\gamma$ such that score is derived from the correct term given the nature of the item's relevance to the user (See lookup table).

\begin{table}\label{BetaLookup}
\begin{center}
	\begin{tabular}{l*{6}{c}r}
		Item type        & $\gamma$ \\
		\hline
		Facebook status  & 0 \\
		Tweet            & 0 \\
		SMS          	 & 0 \\
		Email    		 & 0 \\
		Appointment      & 1 \\
		Task		     & 1 \\
	\end{tabular}
	\caption{$\gamma$ lookup table}
\end{center}
\end{table}

A normalisation constant is introduced in the topic-related term so ensure it is smaller than time-related scores. The maximum value of a topic-score is 10 and there are 12 topics, so the maximum score is 120. The maximum score of the time-related term is 1, giving us the normalisation constant of 1/120.
This gives the normalised weighted scoring function, which incorporates both topic-relevance and time-relevance into a single equation (Eqn. \ref{BasicScoringRule3}).

\begin{equation}\label{BasicScoringRule3}
	f_U (D) = \left[\frac{{\mathrm{e}}^{-\alpha t(d)} \times (1-\gamma) \sum_{k=1}^{m-1} u_kd_k}{120} \right] + \gamma{\mathrm{e}}^{\ (t_{threshold}-t_{tillDue}(D))}
\end{equation}

\subsection{Lexical Analysis and Preprocessing}

The user profile may also include requirements for the filter to remove items which fall under certain criteria. These include readability, sentiment, spam removal, adult content removal, language detection and gender detection. The preprocessor will eliminate items which are excluded by the user profile, before being scored. 

\section{Implementation Progress}

In terms of implementing the algorithm as an API, a test environment has been created for entering test data from the command line and storing them in files as JSON objects. It loads them from files into DataItem objects to be used by the ranking agent. It allows me to chose which types of items are loaded at any point. 

Classes have been implemented which sort items of scored data; get the topic (among other features) of items; store the user's profile; store the item's attribute characteristics and score items according to how well they match the user's profile. 